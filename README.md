# ğŸ”¥ **DVS â€” Disaster Visualization System**

_A journey from chaos to clean, structured, real-time data._

---

## ğŸŒ What This Project Is

DVS is a living system that pulls **satellite fire data** and **earthquake events** in real time, cleans them, stores them in PostgreSQL, and keeps everything automatically deduplicated.

What started as a â€œletâ€™s just store some fire dataâ€ experiment slowly evolved into a full **data ingestion pipeline** â€”  
one that could eventually power a live, interactive disaster dashboard.

Every step â€” the tables, the constraints, the debugging â€” was about making data _flow_ smoothly and truthfully.

---

## ğŸ›°ï¸ The Fire Data Saga

It began with NASAâ€™s **FIRMS** (Fire Information for Resource Management System) API.  
Turns out, FIRMS doesnâ€™t give you _a_ fire list â€” it gives you _many_, each from a different satellite or mode.

I used five of them:

|Source|Description|
|---|---|
|**VIIRS_NOAA20_NRT**|VIIRS sensor on the NOAA-20 satellite (Near Real Time)|
|**VIIRS_NOAA21_NRT**|Same VIIRS sensor, newer satellite|
|**VIIRS_SNPP_NRT**|The Suomi NPP satelliteâ€™s VIIRS feed|
|**MODIS_NRT**|The classic Terra/Aqua MODIS sensors|
|**GOES_NRT**|Geostationary satellites (big, fast coverage)|

At first, I thought merging them all into one table would simplify things.  
It didnâ€™t â€” each behaves differently.

So now, **each satellite has its own table**.  
That decision made everything cleaner: easier debugging, isolated failures, and independent schedules.

---

## ğŸ’¾ The Tables

Each satellite table follows the same schema â€” directly inspired by FIRMSâ€™ CSV format.

|Column|Description|
|---|---|
|`latitude`, `longitude`|Where the fire was detected|
|`bright_ti4`, `bright_ti5`|Thermal band brightness|
|`scan`, `track`|Pixel dimensions (precision)|
|`acq_date`, `acq_time`|When it was captured|
|`satellite`, `instrument`|Source identifiers|
|`confidence`|Detection confidence (L/N/H or %)|
|`version`|Dataset version|
|`frp`|Fire Radiative Power â€” intensity|
|`daynight`|Captured during day or night|

The tricky part?  
FIRMS _re-sends_ the same fire points across updates. Without protection, youâ€™ll get thousands of duplicates.

Thatâ€™s when **PostgreSQL constraints** became my best friend (and worst enemy).

---

## âš™ï¸ The Great Constraint Battle

At first, I assumed this was enough:

`(latitude, longitude, acq_date, acq_time)`

It wasnâ€™t.  
FIRMS happily reuses those but tweaks confidence or FRP.

After many â€œduplicate keyâ€ errors and constraint rebuilds, I landed on the perfect uniqueness set:

`(latitude, longitude, acq_date, acq_time, satellite, instrument, confidence, frp)`

Now, every run quietly ignores repeats.

At one point, I thought the script was inserting 3,000 new rows each loop â€”  
turns out I was just printing the CSV length ğŸ¤¦â€â™‚ï¸.  
The database was calmly rejecting duplicates the whole time.

> Lesson learned: your script might lie, but your database wonâ€™t.

---

## ğŸŒ‹ The Earthquake Side

Next came earthquakes â€” from the **USGS Earthquake Catalog API**.  
Structured, predictable, and refreshingly consistent.

It outputs GeoJSON instead of CSV, which makes parsing simpler.  
Each event comes with its own globally unique ID â€” no deduping headaches.

|Column|Description|
|---|---|
|`id`|Serial primary key|
|`time`|Timestamp of the quake|
|`latitude`, `longitude`, `depth`|Where and how deep|
|`mag`, `magType`|Magnitude and scale|
|`place`|Textual location|
|`status`, `tsunami`, `type`|Event metadata|

No unique constraint needed â€” USGS handles that beautifully.

---

## ğŸ§© Database State

Everything lives inside a single PostgreSQL database, renamed simply to **`dvs`**.

`public â”œâ”€â”€ earthquakes â”œâ”€â”€ firms_viirs_noaa20_nrt â”œâ”€â”€ firms_viirs_noaa21_nrt â”œâ”€â”€ firms_viirs_snpp_nrt â”œâ”€â”€ firms_modis_nrt â””â”€â”€ firms_goes_nrt`

Current size: **~16 MB.**  
Thatâ€™s tiny, considering it stores daily fire detections for an entire continent â€”  
proof that good structure scales gracefully.

---

## ğŸ” The Hidden File

Configuration stays private inside a `.env` file:

`MAP_KEY=my_firms_api_key DB_USER=postgres DB_PASS=passmyword DB_NAME=dvs`

That keeps secrets out of GitHub and your terminal history â€”  
a small practice, but crucial when you automate live ingestion.

---

## ğŸ§  What I Learned

- FIRMS is not one dataset â€” itâ€™s an ecosystem.
    
- Separate tables are sanity savers.
    
- `ON CONFLICT DO NOTHING` is the chillest line of SQL youâ€™ll ever write.
    
- Brightness and FRP together tell deeper stories than coordinates alone.
    
- Numerical data barely eats space; duplication does.
    
- Confidence isnâ€™t computed â€” itâ€™s NASAâ€™s own probability model.
    
- If you think youâ€™re â€œinserting 3000 new rowsâ€â€¦ check your `print()` first ğŸ˜‚.
    

---

## ğŸš€ Next Up

- Combine all feeds visually on a live map (Leaflet/Mapbox).
    
- Add auto-purge logic for older data (30â€“60 days).
    
- Track true insert counts per run.
    
- Expose an internal REST API for visualization layers.
    

---

## ğŸ§¡ The Journey

What began as _â€œletâ€™s store some fire dataâ€_ turned into a self-sustaining ingestion system â€”  
one that fetches, cleans, deduplicates, and stores real NASA and USGS data every few seconds.

Itâ€™s not just about building a database anymore.  
Itâ€™s about **watching the Earth breathe â€” through heat and tremors â€” in real time.**

---

## ğŸŒ Why Hasnâ€™t Anyone Already Done This?

That question stuck with me from the start.  
If NASA, USGS, and ReliefWeb already publish everything,  
why doesnâ€™t the world have a single map that justâ€¦ _shows it all_?

Turns out, the reasons are both **technical** and **human**.

---

### âš™ï¸ 1. The Data Is a Mess

Every disaster type speaks its own language.

|Disaster|Main Source|Format|Typical Update|
|---|---|---|---|
|Earthquakes|USGS|GeoJSON|Every few seconds|
|Wildfires|NASA FIRMS|CSV / GeoTIFF|Every few hours|
|Floods|ReliefWeb|JSON|Irregular|
|Cyclones|NOAA / IMD|XML / shapefile|Every few hours|
|Volcanoes|Smithsonian / USGS|JSON|Irregular|

Different timestamps. Different coordinate systems. Missing fields. Contradictions.  
Most projects die right here â€” data cleaning eats 80% of the timeline _before_ visualization even starts.

---

### â±ï¸ 2. â€œReal-Timeâ€ Isnâ€™t Really Real-Time

NASAâ€™s fire data lags by hours.  
Earthquakes can be revised.  
Cyclones change course mid-feed.

Show too early â€” you spread false alarms.  
Show too late â€” you lose relevance.  
So most dashboards pick one domain: fire, quake, or weather. Never all.

---

### ğŸ”’ 3. APIs Have Limits

Every source imposes:

- Rate limits
    
- Tokens or registration
    
- Quotas per minute/hour
    
- Gigantic payloads
    

You canâ€™t just â€œrefresh everything.â€ You need queues, caching, deduplication â€”  
real infrastructure, not spreadsheets.

---

### ğŸ§© 4. The Cross-Discipline Gap

To build this right, you need to be part:

- Data engineer
    
- Geospatial scientist
    
- Cloud developer
    
- Designer
    

Thatâ€™s four hats few teams wear at once.

---

### ğŸ›ï¸ 5. Institutions Build for Themselves, Not the Public

UN, NASA, NOAA â€” they already monitor everything.  
But their tools are made for **research**, not real-time public insight.

Googleâ€™s _Crisis Map_ (2012-2021) tried to unify it all â€” and worked â€”  
until maintaining live data validation became too costly.

So a vacuum remains:  
all the data exists, but no unified, open, human-readable map connects it.

---

## ğŸ’¡ Why Do It Anyway?

Because now, the hard parts are solvable.

We have APIs, free databases, cloud schedulers, and open data â€”  
the only missing piece is someone who ties them together _cleanly_.

Thatâ€™s where **DVS** comes in.

---

## ğŸŒ My Approach

Instead of chasing the whole planet, I focused on **Asia** â€”  
big, diverse, disaster-prone, and rich with data sources.

By fetching all events within bounding boxes,  
I get precise regional data at low transaction cost.

Each disaster type lives in its own table,  
each table follows a common schema,  
and a single refresh loop keeps everything in sync.

No duplication. No blind inserts. No waiting for grants.  
Just clean, automated data from five satellites and one planet.

---

**Built by Zala Vishmayrajâš¡**  
Because the planet never sleeps â€” and neither should our data.

---
